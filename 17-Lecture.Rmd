---
output:
  html_document: default
  pdf_document: default
---
# Lecture 17 {-}

&nbsp;


## Modeling {-}

### Introduction {-}

```{r, include=FALSE}

library(tidyverse)


dataset1 <- read.csv(file = "C:/Users/alexp/OneDrive/Desktop/2102/Data Sets/lung_capacity_2.csv",
                   
                   header = T,
                   
                   stringsAsFactors = TRUE)


```

Patterns in your data provide clues about relationships among variables. If a systematic relationship exists between two variables it will appear as a pattern in the data. If you spot a pattern, ask yourself:

* Could this pattern be due to coincidence (i.e. random chance)?
* How can you describe the relationship implied by the pattern?
* How strong is the relationship implied by the pattern?
* What other variables might affect the relationship?
* Does the relationship change if you look at individual subgroups of the data?

Patterns provide one of the most useful tools for data analysts because they reveal **covariation**. If you think of **variation** as a phenomenon that creates uncertainty, **covariation** is a phenomenon that reduces it. If two variables covary, you can use the values of one variable to make better predictions about the values of the second. If the covariation is due to a causal relationship (a special case), then you can use the value of one variable to control the value of the second.

**Models** are a tool for extracting patterns out of data. The goal of a model is to provide a simple low-dimensional summary of a dataset. Ideally, the model will capture true “signals” (i.e. patterns generated by the phenomenon of interest), and ignore “noise” (i.e. random variation that you’re not interested in).

In this class we only cover **predictive models**, which, as the name suggests, generate predictions. Specifically, we are going to focus on linear regression models, which are the most famous and arguably the simplest models, but yet they are widely used in all of statistics.  

&nbsp;

### Simple Linear Regression (SLR) Model {-}

As mentioned, a **linear regression model** is one of the simplest models for doing data analysis. Though it may seem somewhat dull compared to some of the more modern statistical learning approaches, linear regression is still a useful and widely applied statistical learning method. 

Moreover, it serves as a good starting point for more advanced approaches; in fact, many of the more sophisticated statistical learning approaches can be seen as generalizations to or extensions of ordinary linear regression. Consequently, it is important to have a good understanding of linear regression before studying more complex learning methods.

Before discussing linear regression models more in detail, we will first give its general definition. We will start off with a **simple linear regression (SLR) model**, a linear model that contains only one predictor.

#### Model Set Up {-}

A simple linear regression model defines a relationship between two variables as follows:

&nbsp;

\begin{align*}
Y_i = \beta_0 + \beta_1 X_i + \epsilon_i \quad  for  \quad  i = 1, \dots, n
\end{align*}

&nbsp;

where

* $Y$ is the **response** (also known as **target**, **outcome**, and **dependent**) variable
* $X$ is the **predictor** (also known as **explanatory** and **independent**) variable and its values are **fixed**
* $\beta_0$ and $\beta_1$ are the **fixed model parameters**; specifically, an **intercept** and **slope** of the regression function, respectively
* $\epsilon$ (epsilon) is an error term and it's a **random variable**; it is assumed to be normally distributed with mean 0 and variance $\sigma^2$, that is, $\epsilon \sim N(0, \sigma^2)$. 

&nbsp;

This model is **linear**, because it uses a linear function to describe a relationship between the response and predictor variables. And it is **simple**, because it contains only one predictor.

&nbsp;

**Interpretation of parameters:** 

The slope parameter $\beta_1$ indicates the change in the mean value of response variable $Y$ per unit increase in the predictor variable $X$.

The intercept parameter $\beta_0$ is the $Y$ intercept of the regression line. When the scope of the model includes $X = 0$, then $\beta_0$ given the mean value of $Y$ at $X = 0$.

&nbsp;

**Model Assumptions:**

Linear regression models must meet certain assumptions to be valid. These assumptions are

* Linearity of the regression function
* Constant variance of the error terms
* Independence of the error terms
* No outliers
* Normality of the error terms

&nbsp;

In this class, we won't focus much on checking these assumptions as it requires covering additional procedures that are beyond the scope of this class. If you are interested in learning more about linear regression models, I highly recommend taking the **UN2103 Applied Linear Regression Analysis** class. Now let's get back to the model. 

If we assume that the relationship between variables can be described using the SLR model, then this model can be utilized to make predictions about the response variable $Y$ based on the values of the predictor variable $X$.

Normally, we make predictions about the mean value of the response variable. In other words, using the predictive models such as SLR, we predict what the average value of the response value would be for a given value of the explanatory variable. It can be written as

&nbsp;

\begin{align*}
E(Y_i) = E(\beta_0 + \beta_1 X_i + \epsilon_i) = \beta_0 + \beta_1 X_i + 0  = \beta_0 + \beta_1 X_i 
\end{align*}

Thus,

\begin{align*}
E(Y_i) = \beta_0 + \beta_1 X_i  \quad  for  \quad  i = 1, \dots, n
\end{align*}

&nbsp;

As you noticed, SLR model contains three parameters: $\beta_0$, $\beta_1$, and $\sigma^2$. These parameters are fixed (they are constant), but their are **unknown** to us. Thus, we first need to estimate these parameters in order to set up a model and make predictions based on this model.

&nbsp;

#### Parameter Estimation: Least Squares Approach {-}

Before we introduce the estimation method, let's consider an example. Recall the lung capacity dataset that we've used in our lectures notes. Suppose you believe that an SLR model can be used to predict _lung capacity_ based on patient's _height_. As adviced earlier, it is always a good idea to visualize your data as a part of EDA (Explanatory Data Analysis):


```{r}

 gg = ggplot(data = dataset1, aes(x = Height, y = LungCap)) + 
  
  geom_point(color = "steelblue") +
  
  labs(title = "Lung Capacity versus Height",
       
       subtitle = "From lung_capacity dataset",
       
       x = "Height",
       
       y = "Lung Capacity")


plot(gg)

```


From the scatterplot it becomes clear that a linear model will indeed be a good fit to our data as it displays a strong linear pattern/trend. Now it's time to estimate the parameters and set up the model. 

But how can we estimate these parameters? In general, our goal is to build a predictive model (known as a **fitted model**) of the form

&nbsp;

\begin{align*}
\hat{Y} = b_0 + b_1X
\end{align*}

&nbsp;

where $b_0$ and $b_1$ are the estimates of the model parameters $\beta_0$ and $\beta_1$, respectively, and $\hat{Y}$ is the **fitted value** of the response variable. There are infinite number of combinations of $b_0$ and $b_1$ values. And these combinations define a family of predictive models.

Let's plot some of them. For example, we could try to model the data with a horizontal line representing the mean value of the _lung capacity_ variable. That is, we are building a model where the response outcome does not depend on the predictor variable:

```{r}

mean_lungcap = mean(dataset1$LungCap)


gg + 

geom_hline(yintercept = mean_lungcap, size = 1)
           
```

In the plot above, we see this doesn’t seem to do a very good job. Many of the data points are very far from the line representing the mean value of the response variable. This is an example of **underfitting**. The obvious fix is to make the fitted model actually depend on the predictor. Below we illustrate a few of these models: 

```{r}

set.seed(4)
models <- tibble(
  b0 = runif(250, -20, 40),
  b1 = runif(250, -5, 5)
)


gg +
  
geom_abline(aes(intercept = b0, slope = b1), data = models)
   

```


Each line on the plot represents a fitted model. There are 250 models on the plot (not all of them are depicted on the plot). Most of these models are really bad! But how are we going to decide which one is the best? And what should be the selection criterion?

We need to find the good models by making precise our intuition that a good model is “close” to the data. We need a way to quantify the distance between the data and a model. Then we can fit the model by finding the value of $b_0$ and $b_1$ that generate the model with the smallest distance from this data.

One easy place to start is to find the vertical distance between each point and the model. This distance is just the difference between the fitted value (predicted) given by the model and the actual response value in the data:

\begin{align*}
Y_i - \hat{Y}_i
\end{align*}


```{r, echo = FALSE}
model <- lm(LungCap ~ Height, data = dataset1)

dataset1$predicted = predict(model)

dataset1$residuals <- residuals(model)

ggplot(dataset1, aes(x = Height, y = LungCap)) +
  
  geom_smooth(method = "lm", se = FALSE, color = "lightgrey") +  
  
  geom_segment(aes(xend = Height, yend = predicted), alpha = .2) + 
  
  geom_point(color = "steelblue") +
  
  geom_point(aes(y = predicted), shape = 1)
  

```

Next, we need some way to compute an overall distance between the predicted and actual values. One common way to do this in statistics is to use the sum of squared deviations (also known as **Error Sum of Squares (SSE)**). We compute the difference between actual and predicted, square them, and add them up:

&nbsp;

\begin{align*}
SSE = \sum_{i = 1}^{n}(Y_i - \hat{Y}_i)^2 = \sum_{i = 1}^{n}(Y_i - (b_0 + b_1X_i))^2
\end{align*}

&nbsp;

This distance has lots of appealing mathematical properties, which we’re not going to talk about here.

Now a pair of values $b_0$ and $b_1$ that minimize the quantity given above are known to be the best estimates of the model parameters $\beta_0$ and $\beta_1$. This approach is known as the **least squares method**, the estimates are called **least squares estimates**, and the fitted model is referred to as **ordinary least squares (OLS) regression**. 

We will skip the derivation of these estimates as it is beyond the scope of this class and simply provide formulas:

&nbsp;

\begin{align*}
b_1 = \frac{\sum_{i = 1}^{n}(x_i - \bar{x})(y_i - \bar{y})}{\sum_{i = 1}^{n}(x_i - \bar{x})^2}
\end{align*}

&nbsp;

\begin{align*}
b_0 = \bar{y} - b_1\bar{x}
\end{align*}

&nbsp;

Now when we know the best estimates of the model parameters, let's calculate their values. First, we will do it manually using the formulas:

```{r}

x <- dataset1$Height

y <- dataset1$LungCap

Sxy <- sum((x - mean(x)) * (y - mean(y)))

Sxx <- sum((x - mean(x)) ^ 2)

Syy <- sum((y - mean(y)) ^ 2)

b_1 <- Sxy / Sxx

print(b_1)

b_0 <- mean(y) - b_1 * mean(x)

print(b_0)
          
```

Thus, a fitted function for our fitted model is given by

&nbsp;

\begin{align*}
\hat{Y} = -13.996 + 0.337 X
\end{align*}


Or, specifically 


\begin{align*}
\hat{Lung Capacity} = -13.996 + 0.337 Height
\end{align*}

&nbsp;

We can interpret the model estimates as follows:

We can expect the mean value of `lung capacity` to increase by 0.337 units per unit increase in the `Height` predictor.

In given context, it makes no sense to interpret the intercept as the scope of the model does not include $X = 0$ (you cannot have a patient with `Height = 0`).


Now, instead of doing calculations manually we can use the `lm()` function:

```{r}

model <- lm(LungCap ~ Height, data = dataset1)

summary(model)

```

The `summary()` function provides a model summary (we will come back to it shortly).

Now we can visualize our results and plot the fitted regression line:

```{r}

gg +
  
  geom_smooth(method = 'lm')

```

Set `se = FALSE` if you want to remove the confidence band around the regression line:

```{r}

gg +
  
  geom_smooth(method = 'lm', se = FALSE)

```

&nbsp;

#### Making Predictions {-}

We can now utilize our fitted model to make predictions. Once again, we first do it manually, then present an R function. Suppose based on the fitted model you want to predict what would the mean lung capacity be for a patient with $Height = 75$?

To do so, we simply plug this value into the model and calculate the corresponding predicted value:

\begin{align*}
\hat{Lung Capacity} = -13.996 + 0.337 Height = -13.996 + 0.337\times 75 = 11.289
\end{align*}

Thus, the expected lung capacity for a patient with `Height = 75` is 11.289. Most of the time, you will want to make predictions for multiple values, and the number of inputs might be quite large. So instead of doing it manually, you can use the `predict()` function. You need to pass the fitted model object to the function and values of the predictor variable that you are trying to make predictions for: 
 
```{r}

predict(model, data.frame(Height = 75))

```


You can even make predictions for multiple values. For example let's calculate the expected mean lung capacity for `Height = 67.5`, `Height = 64`, and `Height = 53.8`:

```{r}

predict(model, data.frame(Height = c(67.5, 64, 53.8)))

```


`predict()` function can also be used to construct confidence interval. We will come back to this function later. 

&nbsp;

#### Residuals {-}

If we think of our model as “Response = Prediction + Error,” we can then write it as

&nbsp;

\begin{align*}
Y = \hat{Y} + e
\end{align*}


We then define a **residual** to be the observed value minus the predicted value:

&nbsp;

\begin{align*}
e = Y - \hat{Y}
\end{align*}


Residuals play a crucial role in a model building process and they are used in many statistical procedures. To obtained residuals for your data, use `residuals()` function (we will display the first 10 residuals as the dataset is large and displaying all residuals will take up much space):


```{r}

# Obtaining first 10 residuals

residuals(model)[1:10]  


```

It is a good idea to bind together the residuals and the predictor variable in one data frame so that you know what residual corresponds to what predictor value (values of the independent variable, **not predictions!**):

```{r}

# Obtaining first 10 residuals and corresponding predictor values

data.frame(Height = dataset1$Height, Residuals = residuals(model))[1:10, ]


```


One of the main properties of residuals is that their summation is equal to zero. This property is used to derive some other important features of linear regression models. Let's show that the residuals for our model indeed add up to zero:

```{r}

sum(residuals(model))

```

&nbsp;

#### Variance Estimation {-}

We will now use the residuals to estimate the model variance, $\sigma^2$. The best estimate of the model variance $\sigma^2$ (derivations once again are omitted) is the **Error Mean Square (MSE)** given by

&nbsp;

\begin{align*}
MSE = \frac{\sum_{i = 1}^{n}(Y_i - \hat{Y}_i)^2}{n-2} = \frac{SSE}{n-2} = \frac{\sum Residuals^2}{n-2}
\end{align*}

&nbsp;

Let's estimate the model variance based on our data. First, let's do it manually:


```{r}

sum(residuals(model)^2)/(dim(dataset1)[1]-2)

```


The output of the `anova()` function includes MSE, that is, an estimate of the model variance. Thus, it can be used for variance estimation:

```{r}

anova(model)

```

In this output the value in the intersection of the `Mean.Sq` column and the `Residuals` row is MSE. Here, it is equal to 1.2.

&nbsp;

#### lm(): Useful Functions {-}

In this section I would like to share with you some useful functions that will allow you extract important information/features from your fitted model.


**summary()**

`summary()` function, as you've already seen, summarizes the fitted model:

```{r}

summary(model)

```

It contains all sorts of information presented in one output. We will discuss them more in detail later on.


**coef()**

If your goal is to extract just the model coefficients (estimates), then use the `coef()` function:

```{r}

coef(model)

```


**resid()**

The `resid()` function is the same as the `residuals()` functions and displays residuals for the fitted model:

```{r}

# Obtaining residuals for the first 10 observations in the dataset

resid(model)[1:10]

```


**fitted()**

The `fitted()` function is used to obtain the fitted values:

```{r}

# Obtaining fitted values for the first 10 observations in the dataset

fitted(model)[1:10]

```


As you understand, there is relationship among the actual response values, fitted values, and the residuals:


```{r}

actual <- dataset1$LungCap[1:10]

fitted <- fitted(model)[1:10]

residuals <- resid(model)[1:10]


data.frame(residuals, actual - fitted)

```
