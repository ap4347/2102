<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>Lecture 19 | STAT 2102: Applied Statistical Computing</title>
<meta name="author" content="Alex Pijyan">
<meta name="description" content="  Resampling Methods There are several steps to creating a useful model, including parameter estimation, model selection and tuning, and performance assessment. At the start of a new project,...">
<meta name="generator" content="bookdown 0.32 with bs4_book()">
<meta property="og:title" content="Lecture 19 | STAT 2102: Applied Statistical Computing">
<meta property="og:type" content="book">
<meta property="og:url" content="https://ap4347.github.io/2102/lecture-19.html">
<meta property="og:description" content="  Resampling Methods There are several steps to creating a useful model, including parameter estimation, model selection and tuning, and performance assessment. At the start of a new project,...">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Lecture 19 | STAT 2102: Applied Statistical Computing">
<meta name="twitter:description" content="  Resampling Methods There are several steps to creating a useful model, including parameter estimation, model selection and tuning, and performance assessment. At the start of a new project,...">
<!-- JS --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://kit.fontawesome.com/6ecbd6c532.js" crossorigin="anonymous"></script><script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="libs/bootstrap-4.6.0/bootstrap.min.css" rel="stylesheet">
<script src="libs/bootstrap-4.6.0/bootstrap.bundle.min.js"></script><script src="libs/bs3compat-0.4.0/transition.js"></script><script src="libs/bs3compat-0.4.0/tabs.js"></script><script src="libs/bs3compat-0.4.0/bs3compat.js"></script><link href="libs/bs4_book-1.0.0/bs4_book.css" rel="stylesheet">
<script src="libs/bs4_book-1.0.0/bs4_book.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- CSS --><style type="text/css">
    
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  </style>
<link rel="stylesheet" href="style.css">
</head>
<body data-spy="scroll" data-target="#toc">

<div class="container-fluid">
<div class="row">
  <header class="col-sm-12 col-lg-3 sidebar sidebar-book"><a class="sr-only sr-only-focusable" href="#content">Skip to main content</a>

    <div class="d-flex align-items-start justify-content-between">
      <h1>
        <a href="index.html" title="">STAT 2102: Applied Statistical Computing</a>
      </h1>
      <button class="btn btn-outline-primary d-lg-none ml-2 mt-1" type="button" data-toggle="collapse" data-target="#main-nav" aria-expanded="true" aria-controls="main-nav"><i class="fas fa-bars"></i><span class="sr-only">Show table of contents</span></button>
    </div>

    <div id="main-nav" class="collapse-lg">
      <form role="search">
        <input id="search" class="form-control" type="search" placeholder="Search" aria-label="Search">
</form>

      <nav aria-label="Table of contents"><h2>Table of contents</h2>
        <ul class="book-toc list-unstyled">
<li><a class="" href="index.html">Welcome</a></li>
<li><a class="" href="lecture-01.html">Lecture 01</a></li>
<li><a class="" href="lecture-02.html">Lecture 02</a></li>
<li><a class="" href="lecture-03.html">Lecture 03</a></li>
<li><a class="" href="lecture-04.html">Lecture 04</a></li>
<li><a class="" href="lecture-05.html">Lecture 05</a></li>
<li><a class="" href="lecture-06.html">Lecture 06</a></li>
<li><a class="" href="lecture-07.html">Lecture 07</a></li>
<li><a class="" href="lecture-08.html">Lecture 08</a></li>
<li><a class="" href="lecture-09.html">Lecture 09</a></li>
<li><a class="" href="lecture-10.html">Lecture 10</a></li>
<li><a class="" href="lecture-11.html">Lecture 11</a></li>
<li><a class="" href="lecture-12.html">Lecture 12</a></li>
<li><a class="" href="lecture-13.html">Lecture 13</a></li>
<li><a class="" href="lecture-14.html">Lecture 14</a></li>
<li><a class="" href="lecture-15.html">Lecture 15</a></li>
<li><a class="" href="lecture-16.html">Lecture 16</a></li>
<li><a class="" href="lecture-17.html">Lecture 17</a></li>
<li><a class="" href="lecture-18.html">Lecture 18</a></li>
<li><a class="active" href="lecture-19.html">Lecture 19</a></li>
</ul>

        <div class="book-extra">
          <p><a id="book-repo" href="https://github.com/ap4347/2102">View book source <i class="fab fa-github"></i></a></p>
        </div>
      </nav>
</div>
  </header><main class="col-sm-12 col-md-9 col-lg-7" id="content"><div id="lecture-19" class="section level1 unnumbered">
<h1>Lecture 19<a class="anchor" aria-label="anchor" href="#lecture-19"><i class="fas fa-link"></i></a>
</h1>
<p> </p>
<div id="resampling-methods" class="section level2 unnumbered">
<h2>Resampling Methods<a class="anchor" aria-label="anchor" href="#resampling-methods"><i class="fas fa-link"></i></a>
</h2>
<p>There are several steps to creating a useful model, including parameter estimation, model selection and tuning, and performance assessment. At the start of a new project, there is usually an initial finite pool of data available for all these tasks, which we can think of as an available data budget. How should the data be applied to different steps or tasks? The idea of <strong>data spending</strong> is an important first consideration when modeling, especially as it relates to empirical validation.</p>
<p>Approaching model-building process correctly means approaching it strategically by spending our data wisely on learning and validation procedures, properly pre-processing the feature and target variables, minimizing <em>data leakage</em>, tuning hyperparameters, and assessing model performance.</p>
<p>In this lecture we will discuss basics of <strong>data splitting</strong>.</p>
<p> </p>
<div id="data-splitting" class="section level3 unnumbered">
<h3>Data Splitting<a class="anchor" aria-label="anchor" href="#data-splitting"><i class="fas fa-link"></i></a>
</h3>
<p>A major goal of any model-building process is to find an algorithm that most accurately predicts future values of the response variable <em>Y</em> based on a set of predictor variables <em>Xs</em>. In other words, we want an algorithm that not only fits well to our past data, but more importantly, one that predicts a future outcome accurately. This is called the <strong>generalizability</strong> of our algorithm. How we “spend” our data will help us understand how well our algorithm generalizes to unseen data.</p>
<p>To provide an accurate understanding of the generalizability of our final optimal model, we can split our data into training and test data sets:</p>
<ul>
<li><p><strong>Training set</strong>: this data is used to develop feature sets, train our algorithms, tune hyperparameters, compare models, and all of the other activities required to choose a final model (e.g., the model we want to put into production).</p></li>
<li><p><strong>Test set</strong>: having chosen a final model, this data is used to estimate an unbiased assessment of the model’s performance, which we refer to as the <em>generalization error</em>.</p></li>
</ul>
<p>Note: it is critical that the test set not be used prior to selecting your final model. Assessing results on the test set prior to final model selection biases the model selection process since the testing data will have become part of the model development process.</p>
<div class="inline-figure"><img src="images/split.jpg"></div>
<p>Given a fixed amount of data, typical recommendations for splitting your data into training-test splits include 60% (training)–40% (testing), 70%–30%, or 80%–20%. Generally speaking, these are appropriate guidelines to follow; however, it is good to keep the following points in mind:</p>
<ul>
<li><p>Spending too much in training (e.g., &gt;80%) won’t allow us to get a good assessment of predictive performance. We may find a model that fits the training data very well, but is not generalizable (<strong>overfitting</strong>).</p></li>
<li><p>Sometimes too much spent in testing ( &gt;40%) won’t allow us to get a good assessment of model parameters (<strong>underfitting</strong>).</p></li>
</ul>
<p>The two most common ways of splitting data include <strong>Simple Random Sampling</strong> and <strong>Stratified Sampling</strong> (both methods were discussed in <strong>Lecture 18</strong>).</p>
<p>Sometimes imbalanced data can have a significant impact on model predictions and performance. Most often this involves classification problems where one class has a very small proportion of observations (e.g., defaults - 5% versus nondefaults - 95%). Several sampling methods have been developed to help remedy class imbalance and most of them can be categorized as either <strong>up-sampling</strong> or <strong>down-sampling</strong>.</p>
<p><strong>Down-sampling</strong> balances the dataset by reducing the size of the abundant class(es) to match the frequencies in the least prevalent class. This method is used when the quantity of data is sufficient. By keeping all samples in the rare class and randomly selecting an equal number of samples in the abundant class, a balanced new dataset can be retrieved for further modeling. Furthermore, the reduced sample size reduces the computation burden imposed by further steps in the model-building process.</p>
<p>On the contrary, <strong>up-sampling</strong> is used when the quantity of data is insufficient. It tries to balance the dataset by increasing the size of rarer samples. Rather than getting rid of abundant samples, new rare samples are generated by using repetition or bootstrapping (discussed later). Note that there is no absolute advantage of one sampling method over another. Application of these two methods depends on the use case it applies to and the data set itself.</p>
<hr>
<p>So far we’ve discussed splitting our data into training and testing sets. Furthermore, we were very explicit about the fact that we <strong>do not</strong> use the test set to assess model performance during the training phase. So how do we assess the generalization performance of the model?</p>
<p><strong>Solution 1:</strong> Assess the model performance based on the training data. Disadvantages - this leads to biased results as some models can perform very well on the training data but not generalize well to a new data set (overfitting).</p>
<p><strong>Solution 2:</strong> Use a validation approach, which involves splitting the training set further to create two parts: a training set and a <strong>validation set</strong> (a.k.a. <strong>holdout set</strong>). Disadvantages - validation using a single holdout set can be highly variable and unreliable unless you are working with very large data sets.</p>
<p><strong>Solution 3:</strong> Resampling methods. They allow to repeatedly fit a model to parts of the training data and test its performance on other parts. The two commonly used resampling methods include <strong>k-fold cross validation</strong> and <strong>bootstrapping</strong>.</p>
<p> </p>
<div id="k-fold-cross-validation" class="section level4 unnumbered">
<h4>k-fold cross validation<a class="anchor" aria-label="anchor" href="#k-fold-cross-validation"><i class="fas fa-link"></i></a>
</h4>
<p>k-fold cross-validation (aka k-fold CV) is a resampling method that randomly divides the training data into <em>k</em> groups (aka folds) of approximately equal size. The model is fit on <em>k-1</em> folds and then the remaining fold is used to compute model performance.</p>
<p>This procedure is repeated <em>k</em> times; each time, a different fold is treated as the validation set. This process results in <em>k</em> estimates of the generalization error. Thus, the k-fold CV estimate is computed by averaging the <em>k</em> test errors, providing us with an approximation of the error we might expect on unseen data.</p>
<div class="inline-figure"><img src="images/folds.jpg" width="100%"></div>
<p>Consequently, with k-fold CV, every observation in the training data will be held out one time to be included in the test set as illustrated in the figure above. In practice, one typically uses <strong>k = 5</strong> or <strong>k = 10</strong>. There is no formal rule as to the size of <em>k</em>; however, as <em>k</em> gets larger, the difference between the estimated performance and the true performance to be seen on the test set will decrease. On the other hand, using too large <em>k</em> can introduce computational burdens</p>
<p>One special variation of cross-validation is <strong>Leave-One-Out (LOO) Cross-Validation</strong>. If there are <em>n</em> training set samples, <em>n</em> models (<em>n</em> folds) are fit using <em>n − 1</em> observations from the training set. Each model predicts the single excluded data point. At the end of resampling, the <em>n</em> predictions are pooled to produce a single performance statistic.</p>
<p>Leave-one-out methods are deficient compared to almost any other method. For anything but pathologically small samples, LOO is computationally excessive, and it may not have good statistical properties.</p>
<p> </p>
</div>
<div id="bootstrapping" class="section level4 unnumbered">
<h4>Bootstrapping<a class="anchor" aria-label="anchor" href="#bootstrapping"><i class="fas fa-link"></i></a>
</h4>
<p>A bootstrap sample is a random sample of the data taken with replacement. This means that, after a data point is selected for inclusion in the subset, it’s still available for further selection. A bootstrap sample is the same size as the original data set from which it was constructed.</p>
<p>The figure below provides a schematic of bootstrap sampling where each bootstrap sample contains 12 observations just as in the original data set. Furthermore, bootstrap sampling will contain approximately the same distribution of values (represented by colors) as the original data set.</p>
<div class="inline-figure"><img src="images/boot.jpg" width="100%"></div>
<p>Since samples are drawn with replacement, each bootstrap sample is likely to contain duplicate values. In fact, on average, approximately 63.21% of the original sample ends up in any particular bootstrap sample. The original observations not contained in a particular bootstrap sample are considered <strong>out-of-bag (OOB)</strong>. When bootstrapping, a model can be built on the selected samples and validated on the OOB samples.</p>
<p> </p>
</div>
</div>
<div id="model-evaluation" class="section level3 unnumbered">
<h3>Model Evaluation<a class="anchor" aria-label="anchor" href="#model-evaluation"><i class="fas fa-link"></i></a>
</h3>
<p>Today, it has become widely accepted that a more sound approach to assessing model performance is to assess the predictive accuracy via <strong>loss functions</strong>. Loss functions are metrics that compare the predicted values to the actual value. When performing resampling methods, we assess the predicted values for a validation set compared to the actual target value.</p>
<p>For example, in regression, one way to measure error is to take the difference between the actual and predicted value for a given observation (this is the usual definition of a residual in ordinary linear regression). The overall validation error of the model is computed by aggregating the errors across the entire validation data set.</p>
<p>There are many loss functions to choose from when assessing the performance of a predictive model, each providing a unique understanding of the predictive accuracy and differing between regression and classification models. Furthermore, the way a loss function is computed will tend to emphasize certain types of errors over others and can lead to drastic differences in how we interpret the “optimal model”.</p>
<p>Its important to consider the problem context when identifying the preferred performance metric to use. And when comparing multiple models, we need to compare them across the same metric.</p>
<p>Depending on the problem context, we can divide them into two groups: <strong>Regression</strong> and <strong>Classification</strong> problems.</p>
<p> </p>
<div id="regression-metrics" class="section level4 unnumbered">
<h4>Regression Metrics<a class="anchor" aria-label="anchor" href="#regression-metrics"><i class="fas fa-link"></i></a>
</h4>
<ul>
<li><p><strong>MSE - Mean Square Error:</strong> Mean squared error is the average of the squared error (<span class="math inline">\(MSE = \frac{\sum_{i=1}^{n}(y_i - \hat{y_i})^2}{n}\)</span>). The squared component results in larger errors having larger penalties. This (along with RMSE) is the most common error metric to use. <strong>Objective: minimize</strong></p></li>
<li><p><strong>RMSE - Root Mean Square Root:</strong> Root mean squared error. This simply takes the square root of the MSE metric (<span class="math inline">\(RMSE =\sqrt{ \frac{\sum_{i=1}^{n}(y_i - \hat{y_i})^2}{n}}\)</span>) so that your error is in the same units as your response variable. If your response variable units are dollars, the units of MSE are dollars-squared, but the RMSE will be in dollars. <strong>Objective: minimize</strong></p></li>
<li><p><strong><span class="math inline">\(R^2\)</span>:</strong> This is a popular metric that represents the proportion of the variance in the dependent variable that is predictable from the independent variable(s). Unfortunately, it has several limitations. For example, two models built from two different data sets could have the exact same RMSE but if one has less variability in the response variable then it would have a lower <span class="math inline">\(R^2\)</span> than the other. You should not place too much emphasis on this metric. <strong>Objective: maximize</strong></p></li>
</ul>
<p> </p>
</div>
<div id="classification-metrics" class="section level4 unnumbered">
<h4>Classification Metrics<a class="anchor" aria-label="anchor" href="#classification-metrics"><i class="fas fa-link"></i></a>
</h4>
<p>Before we discuss evaluation metrics for classification problems, I would like to introduce a <strong>confusion matrix</strong>. In the field of statistics and machine learning, a confusion matrix is a table that visualizes the performance of a model. Each row of the confusion matrix represents the instances in an actual class while each column represents the instances in a predicted class, or vice versa (both variants are accepted and found in the literature).</p>
<p>It is a special kind of contingency table, with two dimensions (“actual” and “predicted”), and identical sets of “classes” in both dimensions (each combination of dimension and class is a variable in the contingency table):</p>
<div class="inline-figure"><img src="images/conf.jpg" width="100%"></div>
<ul>
<li>
<strong>Accuracy:</strong> accuracy tells you, overall, how often the classifier is correct.</li>
</ul>
<p> </p>
<p><span class="math display">\[\begin{align*}
Accuracy = \frac{TP + TN}{Total}
\end{align*}\]</span></p>
<p> </p>
<div class="inline-figure"><img src="images/conf2.jpg" width="100%"></div>
<ul>
<li>
<strong>Sensitivity (aka Recall):</strong> sensitivity tells you how accurately the classifier classifies actual events.</li>
</ul>
<p> </p>
<p><span class="math display">\[\begin{align*}
Sensitivity = \frac{TP}{TP + FN}
\end{align*}\]</span></p>
<p> </p>
<ul>
<li>
<strong>Specificity:</strong> specificity tells you how accurately the classifier classifies actual non-events.</li>
</ul>
<p> </p>
<p><span class="math display">\[\begin{align*}
Specificity = \frac{TN}{FP + TN}
\end{align*}\]</span></p>
<p> </p>
<ul>
<li>
<strong>Precision:</strong> precision tells you how accurately the classifier predicts actual events.</li>
</ul>
<p> </p>
<p><span class="math display">\[\begin{align*}
Precision = \frac{TP}{TP + FP}
\end{align*}\]</span></p>
<p> </p>

</div>
</div>
</div>
</div>
  <div class="chapter-nav">
<div class="prev"><a href="lecture-18.html">Lecture 18</a></div>
<div class="empty"></div>
</div></main><div class="col-md-3 col-lg-2 d-none d-md-block sidebar sidebar-chapter">
    <nav id="toc" data-toggle="toc" aria-label="On this page"><h2>On this page</h2>
      <ul class="nav navbar-nav">
<li><a class="nav-link" href="#lecture-19">Lecture 19</a></li>
<li>
<a class="nav-link" href="#resampling-methods">Resampling Methods</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#data-splitting">Data Splitting</a></li>
<li><a class="nav-link" href="#model-evaluation">Model Evaluation</a></li>
</ul>
</li>
</ul>

      <div class="book-extra">
        <ul class="list-unstyled">
<li><a id="book-source" href="https://github.com/ap4347/2102/blob/master/19-Lecture.Rmd">View source <i class="fab fa-github"></i></a></li>
          <li><a id="book-edit" href="https://github.com/ap4347/2102/edit/master/19-Lecture.Rmd">Edit this page <i class="fab fa-github"></i></a></li>
        </ul>
</div>
    </nav>
</div>

</div>
</div> <!-- .container -->

<footer class="bg-primary text-light mt-5"><div class="container"><div class="row">

  <div class="col-12 col-md-6 mt-3">
    <p>"<strong>STAT 2102: Applied Statistical Computing</strong>" was written by Alex Pijyan. It was last built on 2023-04-18.</p>
  </div>

  <div class="col-12 col-md-6 mt-3">
    <p>This book was built by the <a class="text-light" href="https://bookdown.org">bookdown</a> R package.</p>
  </div>

</div></div>
</footer><!-- dynamically load mathjax for compatibility with self-contained --><script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script><script type="text/x-mathjax-config">const popovers = document.querySelectorAll('a.footnote-ref[data-toggle="popover"]');
for (let popover of popovers) {
  const div = document.createElement('div');
  div.setAttribute('style', 'position: absolute; top: 0, left:0; width:0, height:0, overflow: hidden; visibility: hidden;');
  div.innerHTML = popover.getAttribute('data-content');

  var has_math = div.querySelector("span.math");
  if (has_math) {
    document.body.appendChild(div);
    MathJax.Hub.Queue(["Typeset", MathJax.Hub, div]);
    MathJax.Hub.Queue(function() {
      popover.setAttribute('data-content', div.innerHTML);
      document.body.removeChild(div);
    })
  }
}
</script>
</body>
</html>
